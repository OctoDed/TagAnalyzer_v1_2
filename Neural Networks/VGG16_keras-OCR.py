# -*- coding: utf-8 -*-
"""Копия блокнота "square box cropped"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Upx1mt5-V6dhr6uNT7k8mkfbhleUoEpx
"""

import torch
import random
from PIL import Image
import numpy as np
from numpy import int64
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from google.colab import files
from google.colab import drive
import os
import glob
drive.mount('/gdrive')
root = '/gdrive/My Drive/'

import tensorflow as tf
import tensorflow_datasets as tfds
from IPython.display import clear_output

tf.test.gpu_device_name()

#write your own ways on google drive or your local PC

data_root = '/gdrive/My Drive/GPO/dataset/'
test_path = "/gdrive/My Drive/GPO/dataset/test/tags/"
train_img_path = "/gdrive/My Drive/GPO/dataset/train/tags/"
train_mask_path = "/gdrive/My Drive/GPO/dataset/train/price_card_mask/" #
test_mask_path = "/gdrive/My Drive/GPO/dataset/test/price_card_mask/" #
'''
data_root = '/gdrive/MyDrive/ГПО/Датасет/dataset 512/dataset/'
test_path = "/gdrive/MyDrive/ГПО/Датасет/dataset 512/dataset/test/tags/"
train_img_path = "/gdrive/MyDrive/ГПО/Датасет/dataset 512/dataset/train/tags/"
train_mask_path = "/gdrive/MyDrive/ГПО/Датасет/dataset 512/dataset/train/price_card_mask/" #
test_mask_path = "/gdrive/MyDrive/ГПО/Датасет/dataset 512/dataset/test/price_card_mask/" #
'''
train_dir = 'tags'
mask_dir = 'price_card_mask'
test_dir = 'test'
random.seed(0)
np.random.seed(0)

!pip install imageio

from imageio import imread
import cv2
import numpy as np
from google.colab.patches import cv2_imshow
#x - tags, у - masks

x1 = []
x2 = []
y1 = []
y2 = []

#you can get dataset via google drive: https://drive.google.com/file/d/13m6woHitEQHrvWEwvP3oL4o-CQULC0vv/view?usp=sharing

for fl1 in sorted(os.listdir(train_img_path)):
    img =np.array(imread(train_img_path + fl1))
    img = tf.convert_to_tensor(img)
    img = tf.cast(img, tf.float32) / 255.0
    #img = tf.image.crop_to_bounding_box(img, 127, 127, 128, 128)
    x1.append(img)
for fl2 in sorted(os.listdir(train_mask_path)):
    mask = cv2.imread(train_mask_path + fl2,0)
    ret,thresh = cv2.threshold(mask,127,255,0)
    contours,hierarchy = cv2.findContours(thresh, 1, 2)
    cnt = contours[0]
    x,y,w,h = cv2.boundingRect(cnt)
    y1.append(tf.convert_to_tensor(np.array([y, x, h, w])/256))
for fl3 in sorted(os.listdir(test_path)):
    test_img = imread(test_path + fl3)
    test_img = tf.convert_to_tensor(np.array(test_img))
    test_img = tf.cast(test_img, tf.float32) / 255.0
    #test_img = tf.image.crop_to_bounding_box(test_img, 127, 127, 128, 128)
    x2.append(test_img)
for fl4 in sorted(os.listdir(test_mask_path)):
    mask = cv2.imread(test_mask_path + fl4,0)
    ret,thresh = cv2.threshold(mask,127,255,0)
    contours,hierarchy = cv2.findContours(thresh, 1, 2)
    cnt = contours[0]
    x,y,w,h = cv2.boundingRect(cnt)
    y2.append(tf.convert_to_tensor(np.array([y, x, h, w])/256))

import cv2
import numpy as np
from google.colab.patches import cv2_imshow

img_o = None
mask = None
for fl1 in sorted(os.listdir(train_img_path)):
    img_o = cv2.imread(train_img_path + fl1)
    break
for fl2 in sorted(os.listdir(train_mask_path)):
    mask = cv2.imread(train_mask_path + fl2,0)
    ret,thresh = cv2.threshold(mask,127,255,0)
    contours,hierarchy = cv2.findContours(thresh, 1, 2)
    cnt = contours[0]
    x,y,w,h = cv2.boundingRect(cnt)
    print(x, y, w, h)
    img = tf.image.crop_to_bounding_box(x1[0], y, x, h, w)
    box = np.array([y, x, y+h, x+w])/256
    boxes = box.reshape([1, 1, 4])
    colors = np.array([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])
    print(tf.expand_dims(x1[0], axis=0).shape)
    img = tf.image.draw_bounding_boxes(tf.expand_dims(x1[0], axis=0), boxes, colors)
    break
cv2_imshow(mask)
plt.imshow(img[0])

train = tf.data.Dataset.from_tensor_slices((x1, y1))
test = tf.data.Dataset.from_tensor_slices((x2, y2))
train_dataset = train.batch(5)
test_dataset = test.batch(1)

len(train)

def display(display_list):
  plt.figure(figsize=(15, 15))

  title = ['Input Image', 'True Mask', 'Predicted Mask']

  for i in range(len(display_list)):
    plt.subplot(1, len(display_list), i+1)
    plt.title(title[i])
    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))
    plt.axis('off')
  plt.show()

print(train.take(1))

for kappa in range(1):
  for image, mask in train.take(kappa):
    sample_image, sample_mask = image, mask
    y, x, h, w = sample_mask.numpy()
    box = np.array([y, x, y+h, x+w])
    boxes = box.reshape([1, 1, 4])
    colors = np.array([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])
    mask = tf.image.draw_bounding_boxes(tf.expand_dims(sample_image, axis=0), boxes, colors)[0]
  print(sample_image.shape)
  print(mask.shape)
  display([sample_image, mask])

for kappa in range(1, 10):
  for image, mask in test.take(kappa):
    sample_image, sample_mask = image, mask
    y, x, h, w = sample_mask.numpy()
    box = np.array([y, x, y+h, x+w])
    boxes = box.reshape([1, 1, 4])
    colors = np.array([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])
    mask = tf.image.draw_bounding_boxes(tf.expand_dims(sample_image, axis=0), boxes, colors)[0]
  print(sample_image.shape)
  print(mask.shape)
  display([sample_image, mask])

# import the necessary packages
from tensorflow.keras.applications import VGG16, ResNet152V2, NASNetLarge
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.preprocessing.image import load_img
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras import backend as K
import numpy as np
import cv2
import os

# load model and specify a new input shape for images
new_input = Input(shape=(256, 256, 3))
vgg = VGG16(include_top=False, input_tensor=new_input)
vgg.trainable = False
flatten = vgg.output
flatten = Flatten()(flatten)
bboxHead = Dense(4096, activation="relu")(flatten)
bboxHead = Dense(2048, activation="relu")(bboxHead)
bboxHead = Dense(1024, activation="relu")(bboxHead)
bboxHead = Dense(512, activation="relu")(bboxHead)
bboxHead = Dense(256, activation="relu")(bboxHead)
bboxHead = Dense(128, activation="relu")(bboxHead)
bboxHead = Dropout(0.1)(bboxHead)
bboxHead = Dense(4)(bboxHead)
# construct the model we will fine-tune for bounding box regression
model = Model(inputs=vgg.input, outputs=bboxHead)
model.summary()

tf.keras.utils.plot_model(model, show_shapes=True)

from keras import backend as K

def dice_coef (y_true, y_pred, smooth=1):
  y_true_f = K.klatten(y_true)
  y_pred_f = K.flatten(y_pred)
  intersection = K.sum(y_true_f * y_pred_f)
  return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def IoU(y_true, y_pred):
  y_true_f = K.klatten(y_true)
  y_pred_f = K.flatten(y_pred)
  intersection = K.sum(y_true_f * y_pred_f)
  union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection
  return intersection/union

optim = tf.keras.optimizers.Adagrad()
model.compile(optimizer=optim,
              loss=tf.keras.losses.MeanAbsolutePercentageError(),
              #loss=tf.keras.losses.LogCosh(),
              metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])
              #metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])

'''
model.compile(optimizer='sgd',
              loss=tf.keras.losses.MeanAbsolutePercentageError(),
              metrics=[tf.keras.metrics.MeanAbsoluteError()])
'''

def create_mask(pred_mask):
  pred_mask = tf.argmax(pred_mask, axis=-1)
  pred_mask = pred_mask[..., tf.newaxis]
  return pred_mask[0]

coord_array=[]

def show_predictions(dataset=None, num=1):
  if dataset:
    for image, mask in dataset.take(num):
      print(num)
      pred_mask = model.predict(image)
      y, x, h, w = pred_mask[0]
      box = np.array([y, x, y+h, x+w])
      boxes = box.reshape([1, 1, 4])
      colors = np.array([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])
      predicted = tf.image.draw_bounding_boxes(tf.expand_dims(image[0], axis=0), boxes, colors)[0]
      y, x, h, w = mask[0].numpy()
      box = np.array([y, x, y+h, x+w])
      print(box)
      boxes = box.reshape([1, 1, 4])
      colors = np.array([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])
      mask_box = tf.image.draw_bounding_boxes(tf.expand_dims(image[0], axis=0), boxes, colors)[0]
      display([image[0], mask_box, predicted])
  else:
    pred_mask = model.predict(sample_image)
    y, x, h, w = pred_mask[0]
    box = np.array([y, x, y+h, x+w])
    boxes = box.reshape([1, 1, 4])
    colors = np.array([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])
    predicted = tf.image.draw_bounding_boxes(tf.expand_dims(sample_image, axis=0), boxes, colors)[0]
    y, x, h, w = sample_mask.numpy()
    box = np.array([y, x, y+h, x+w])/255
    boxes = box.reshape([1, 1, 4])
    colors = np.array([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])
    mask_box = tf.image.draw_bounding_boxes(tf.expand_dims(sample_image, axis=0), boxes, colors)[0]
    display([sample_image, mask_box,
             predicted])

class DisplayCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    clear_output(wait=True)
    show_predictions()
    print ('\nSample Prediction after epoch {}\n'.format(epoch+1))

EPOCHS = 50
vgg.trainable = True
model_history = model.fit(train_dataset, epochs=EPOCHS,
                          validation_data=test_dataset)

loss = model_history.history['loss']
val_loss = model_history.history['val_loss']

epochs = range(len(loss))

plt.figure()
plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss Value')
plt.ylim([0, 100])
plt.legend()
plt.show()

"""LOADING THE WEIGHTS"""

#you can get pre-trained weights via google drive: https://drive.google.com/file/d/1XRiR_AM4IHvVae_rEUh-avA_s8AbZwKN/view?usp=sharing

model.load_weights('/gdrive/MyDrive/GPO/weights/VGG16/vgg16_1.h5')

"""RESAULTS"""

show_predictions(test_dataset, 21)

"""SAVING THE WEIGHTS"""

model.save_weights('/gdrive/MyDrive/GPO/weights/VGG16/vgg16_256.h5')

"""CRAFT"""

!pip install keras-ocr

from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from imageio import imread

from keras.preprocessing.image import save_img
k = 0

crop_array=[]
def show_predictions_cropping(dataset=None, num=1):
  k=0
  if dataset:
    for image, mask in dataset.take(num):
      pred_mask = model.predict(image)
      y, x, h, w = pred_mask[0]
      y1, x1, h1, w1 = pred_mask[0]
      print(round(y*256), round(x*256), round(h*256), round(w*256))
      box = np.array([y, x, y+h, x+w])
      boxes = box.reshape([1, 1, 4])
      colors = np.array([[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])
      predicted = tf.image.draw_bounding_boxes(tf.expand_dims(image[0], axis=0), boxes, colors)[0]
      y, x, h, w = mask[0].numpy()
      box = np.array([y, x, y+h, x+w])
      print(box)
      crop_array.append(box)
      boxes = box.reshape([1, 1, 4])
      colors = np.array([[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])
      mask_box = tf.image.draw_bounding_boxes(tf.expand_dims(image[0], axis=0), boxes, colors)[0]
      print(boxes)
      cropped_image = tf.image.crop_to_bounding_box(image[0], round(y1*256), round(x1*256), round(h1*256), round(w1*256))
      save_img("/gdrive/MyDrive/GPO/dataset/croped4/"+str(k)+"_cropped.jpg", cropped_image) #create directory and change it's way before starting this cell
      k=k+1
      display([cropped_image])
  else:
    pred_mask = model.predict(sample_image)
    y, x, h, w = pred_mask[0]
    box = np.array([y, x, y+h, x+w])
    boxes = box.reshape([1, 1, 4])
    colors = np.array([[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])
    predicted = tf.image.draw_bounding_boxes(tf.expand_dims(sample_image, axis=0), boxes, colors)[0]
    y, x, h, w = sample_mask.numpy()
    box = np.array([y, x, y+h, x+w])
    boxes = box.reshape([1, 1, 4])
    colors = np.array([[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])
    mask_box = tf.image.draw_bounding_boxes(tf.expand_dims(sample_image, axis=0), boxes, colors)[0]
    display([sample_image, mask_box,
             predicted])

show_predictions_cropping(test_dataset, 21)

import keras_ocr
import string
data_dir = '/gdrive/MyDrive/GPO/'

#alphabet = string.digits + string.ascii_letters + '!?. '
#alphabet = string.digits + string.ascii_letters + string.punctuation + 'АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя'
#alphabet = string.digits + string.ascii_letters + '!?.' + 'БбвГгДдЁёЖжЗзИиЙйЛлПпФфЦцЧчШшЩщЪъЫыЬьЭэЮюЯя'
#alphabet = string.digits + 'АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя'
alphabet = string.digits
recognizer_alphabet = alphabet
#recognizer_alphabet = ''.join(sorted(set(alphabet.lower())))
fonts = keras_ocr.data_generation.get_fonts(
    alphabet=alphabet,
    cache_dir=data_dir
)

detector = keras_ocr.detection.Detector(weights='clovaai_general')
recognizer = keras_ocr.recognition.Recognizer(
    alphabet=recognizer_alphabet
    #weights='kurapan'
)
recognizer.compile()

for layer in recognizer.backbone.layers:
    layer.trainable = False

#you can get pre-trained weights via google drive:
# 1 https://drive.google.com/file/d/19NgEhS_ckwP-VRNrBoc73n554NuVXHjv/view?usp=sharing - end-to-end learning, digits only
# 2 https://drive.google.com/file/d/1rbNdNSWi1JkM2JgGzdopUqEyu1IFGtYg/view?usp=sharing - transfer learning, digits only
# 3 https://drive.google.com/file/d/1fA1KlwM5vWGUNjbyiUZuI7p8BmOo0aW5/view?usp=sharing - end-to-end learning, digits and rus
# 4 https://drive.google.com/file/d/1NVOxXCPQ_KOMZFlSmdVsw2R8SfcllvJd/view?usp=sharing - transfer learning, digits and rus
# 5 https://drive.google.com/file/d/1pq8aP2sn5CQW6yzipge5nCFygcnSpWUL/view?usp=sharing - transfer learning, digits, eng and rus

recognizer.model.load_weights('/gdrive/MyDrive/colab/keras-ocr/digits(full learning)_256.h5')

# keras-ocr will automatically download pretrained
# weights for the detector and recognizer.
pipeline = keras_ocr.pipeline.Pipeline(detector=detector, recognizer=recognizer)
cropped_img_path = "/gdrive/MyDrive/GPO/dataset/croped4/"

# Get a set of three example images

images1 = []
for fl5 in sorted(os.listdir(cropped_img_path)):
    img = cropped_img_path+fl5
    images1.append(img)


images = [
    keras_ocr.tools.read(img) for img in images1
]

# Each list of predictions in prediction_groups is a list of
# (word, box) tuples.
prediction_groups = pipeline.recognize(images)

# Plot the predictions
fig, axs = plt.subplots(nrows=len(images), figsize=(20, 20))
for ax, image, predictions in zip(axs, images, prediction_groups):
    keras_ocr.tools.drawAnnotations(image=image, predictions=predictions, ax=ax)

print(pipeline.recognize(images))